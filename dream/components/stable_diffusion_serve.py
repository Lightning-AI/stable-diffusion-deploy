import base64
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from dataclasses import dataclass
from io import BytesIO
import time
from typing import List
import uuid

import lightning as L
import numpy as np
import torch
from fastapi import HTTPException
from lightning.app.structures import List as LightningList
from lightning.app.api import Post
from lightning_app.storage import Drive
from PIL import Image
from torch import autocast
from pydantic import BaseModel


REQUEST_TIMEOUT = 5 * 60


# @dataclass
# class FastAPIBuildConfig(L.BuildConfig):
#     requirements = ["fastapi==0.78.0", "uvicorn==0.17.6"]


class ModelInference(L.LightningWork):
    def __init__(self, **kwargs):
        super().__init__(cloud_compute=L.CloudCompute("gpu"), cache_calls=True, parallel=True, **kwargs)
        self._model = None
        self.busy = False
        self.results = []

    def build_model(self):
        """The `build_model(...)` method returns a model and the returned model is set to `self._model` state."""
        import os

        import torch
        from diffusers import StableDiffusionPipeline

        access_token = os.environ.get("access_token")

        # make sure you're logged in with `huggingface-cli login`
        print("loading model...")
        if torch.cuda.is_available():
            pipe = StableDiffusionPipeline.from_pretrained(
                "CompVis/stable-diffusion-v1-4",
                revision="fp16",
                torch_dtype=torch.float16,
                use_auth_token=access_token,
            )
            pipe = pipe.to("cuda")
            print("model loaded")
        else:
            pipe = None
            print("model set to None")
        return pipe

    def predict(self, dream: str, num_images: int, image_size: int):
        if self._model is None:
            self._model = self.build_model()
        height, width = image_size, image_size
        prompts = [dream] * int(num_images)
        pil_results = []
        with autocast("cuda"):
            # predicting in chunks to save cuda out of memory error
            chunk_size = 3
            for i in range(0, num_images, chunk_size):
                if torch.cuda.is_available():
                    pil_results.extend(self._model(prompts[i : i + chunk_size], height=height, width=width)["sample"])
                else:
                    pil_results.extend([Image.fromarray(np.random.randint(0, 255, (height, width, 3), dtype="uint8"))])

        # results = []
        for image in pil_results:
            # buffered = BytesIO()
            image.save(buffered, format="PNG")
            # img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            # results.append(f"data:image/png;base64,{img_str}")
        # return results

    def run(self, uuid: str, dream: str, num_images: int, image_size: int):
        self.busy = True
        if self._model is None:
            self._model = self.build_model()
        results = self.predict(uuid, dream, num_images, image_size)
        self.busy = False


# class StableDiffusionServeAPI(L.LightningWork):
#     """Deploys the Stable Diffusion model with FastAPI."""
# 
#     def __init__(self, **kwargs):
#         super().__init__(**kwargs)
#         self.requests = []
#         self.drive = Drive("lit://results")
# 
#     def run(self, *args, **kwargs) -> None:
#         import uvicorn
#         from fastapi import FastAPI
#         from fastapi.middleware.cors import CORSMiddleware
# 
#         app = FastAPI()
# 
#         app.add_middleware(
#             CORSMiddleware,
#             allow_origins=["*"],
#             allow_credentials=True,
#             allow_methods=["*"],
#             allow_headers=["*"],
#         )
# 
# 
#         @app.post("/api/predict/")
#         async def predict_api(data: Data):
#             """Dream a dream. Defines the REST API which takes the text prompt, number of images and image size in the
#             request body.
# 
#             This API returns an image generated by the model in base64 format.
#             """
#             try:
#                 result = pool.submit(self.predict, data.dream, data.num_images, data.image_size).result(
#                     timeout=REQUEST_TIMEOUT
#                 )
#                 return result
#             except TimeoutError:
#                 raise HTTPException(status_code=500, detail="Request timed out.")
# 
#         uvicorn.run(app, host=self.host, port=self.port)


class Data(BaseModel):
    dream: str
    num_images: int
    image_size: int


class StableDiffusionServe(L.LightningFlow):
    """Serves the Stable Diffusion model."""

    def __init__(self, num_workers=2, **kwargs):
        super().__init__(**kwargs)

        self.workers: List[ModelInference] = LightningList(*[ModelInference() for _ in range(num_workers)])
        self.num_workers = num_workers
        self.requests = []

    # def predict(self, dream, num_image, image_size):
    #     while True:
    #         for worker in self._workers:
    #             if not worker.busy:
    #                 return worker.predict(dream, num_image, image_size)
    #         sleep(0.05)
    #         print("All workers busy!")

    def handle_predict(self, data: Data):
        job_uuid = uuid.uuid4()
        self.requests.append([job_uuid, data.dream, data.num_images, data.image_size])

        # Look into results from the work
        # issue is, how do we remove those results from the work?

        start = time.time()
        while True:
            if job_uuid in self._results:
                res = self._results.pop(job_uuid)
                return res
            if time.time() - start > REQUEST_TIMEOUT:
                break

        # raise HTTPException(status_code=500, detail="Request timed out.")

    def configure_api(self):
        return [Post(route="/api/predict", method=self.handle_predict)]

    def run(self, *args, **kwargs) -> None:
        # self.predict(data.dream, data.num_images, data.image_size)
        if not self.requests:
            return

        job_uuid, *inputs = self.requests[0]
        results = None

        for worker in self.workers:
            if not worker.busy:
                self.requests.pop(0)
                worker.run(inputs)
                self.results[] = worker.result
                break

        # if self.workers[0]
        # self.workers[0].run(request)


# class StableDiffusionServe_(L.LightningWork):
#     """Deploys the Stable Diffusion model with FastAPI."""
# 
#     def __init__(self, num_workers=2, **kwargs):
#         super().__init__(**kwargs)
# 
#         self._workers: List[ModelInference] = LightningList(*[ModelInference() for _ in range(num_workers)])
#         self.num_workers = num_workers
# 
#     def predict(self, dream, num_image, image_size):
#         while True:
#             for worker in self._workers:
#                 if not worker.busy:
#                     return worker.predict(dream, num_image, image_size)
#             sleep(0.05)
#             print("All workers busy!")
# 
#     def run(self, *args, **kwargs) -> None:
#         import uvicorn
#         from fastapi import FastAPI
#         from fastapi.middleware.cors import CORSMiddleware
#         from pydantic import BaseModel
# 
#         for worker in self._workers:
#             worker.run()
# 
#         pool = ThreadPoolExecutor(max_workers=self.num_workers)
#         app = FastAPI()
# 
#         app.add_middleware(
#             CORSMiddleware,
#             allow_origins=["*"],
#             allow_credentials=True,
#             allow_methods=["*"],
#             allow_headers=["*"],
#         )
# 
#         class Data(BaseModel):
#             dream: str
#             num_images: int
#             image_size: int
# 
#         @app.post("/api/predict/")
#         async def predict_api(data: Data):
#             """Dream a dream. Defines the REST API which takes the text prompt, number of images and image size in the
#             request body.
#             This API returns an image generated by the model in base64 format.
#             """
#             try:
#                 result = pool.submit(self.predict, data.dream, data.num_images, data.image_size).result(
#                     timeout=REQUEST_TIMEOUT
#                 )
#                 return result
#             except TimeoutError:
#                 raise HTTPException(status_code=500, detail="Request timed out.")
# 
#         uvicorn.run(app, host=self.host, port=self.port)