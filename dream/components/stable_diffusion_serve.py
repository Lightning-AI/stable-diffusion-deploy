import base64
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from dataclasses import dataclass
from io import BytesIO
from time import sleep
from typing import List

import lightning as L
import numpy as np
import torch
from fastapi import HTTPException
from lightning.app.structures import List as LightningList
from PIL import Image
from torch import autocast

REQUEST_TIMEOUT = 5 * 60


@dataclass
class FastAPIBuildConfig(L.BuildConfig):
    requirements = ["fastapi==0.78.0", "uvicorn==0.17.6"]


class ModelInference(L.LightningWork):
    def __init__(self, **kwargs):
        super().__init__(cloud_compute=L.CloudCompute("gpu"), cache_calls=True, parallel=True, **kwargs)
        self._model = None
        self._busy = False

    @property
    def busy(self):
        return self._busy

    def build_model(self):
        """The `build_model(...)` method returns a model and the returned model is set to `self._model` state."""
        import os

        import torch
        from diffusers import StableDiffusionPipeline

        access_token = os.environ.get("access_token")

        # make sure you're logged in with `huggingface-cli login`
        print("loading model...")
        if torch.cuda.is_available():
            pipe = StableDiffusionPipeline.from_pretrained(
                "CompVis/stable-diffusion-v1-4",
                revision="fp16",
                torch_dtype=torch.float16,
                use_auth_token=access_token,
            )
            pipe = pipe.to("cuda")
            print("model loaded")
        else:
            pipe = None
            print("model set to None")
        return pipe

    def predict(self, dream: str, num_images: int, image_size: int):
        self._busy = True
        if self._model is None:
            self._model = self.build_model()
        height, width = image_size, image_size
        prompts = [dream] * int(num_images)
        pil_results = []
        with autocast("cuda"):
            # predicting in chunks to save cuda out of memory error
            chunk_size = 3
            for i in range(0, num_images, chunk_size):
                if torch.cuda.is_available():
                    pil_results.extend(self._model(prompts[i : i + chunk_size], height=height, width=width)["sample"])
                else:
                    pil_results.extend([Image.fromarray(np.random.randint(0, 255, (height, width, 3), dtype="uint8"))])

        results = []
        for image in pil_results:
            buffered = BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            results.append(f"data:image/png;base64,{img_str}")
        self._busy = False
        return results

    def run(self, *args, **kwargs):
        self._model = self.build_model()


class ScaleModel(L.LightningFlow):
    def __init__(self, num_workers):
        super().__init__()
        self.num_workers = num_workers

        self._workers: List[ModelInference] = LightningList(*[ModelInference() for _ in range(num_workers)])

    def predict(self, dream, num_image, image_size):
        while True:
            for worker in self._workers:
                if not worker.busy:
                    return worker.predict(dream, num_image, image_size)
            sleep(0.05)
            print("All workers busy!")

    def run(self):
        for worker in self._workers:
            worker.run()


class StableDiffusionServe(L.LightningWork):
    """Deploys the Stable Diffusion model with FastAPI."""

    def __init__(self, num_workers=2, **kwargs):
        super().__init__(**kwargs)

        self._scaled_model: ScaleModel = None
        self.num_workers = num_workers

    def run(self, *args, **kwargs) -> None:
        import uvicorn
        from fastapi import FastAPI
        from fastapi.middleware.cors import CORSMiddleware
        from pydantic import BaseModel

        pool = ThreadPoolExecutor(max_workers=self.num_workers)
        if self._scaled_model is None:
            self._scaled_model = ScaleModel(self.num_workers)
        self._scaled_model.run()

        app = FastAPI()

        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

        class Data(BaseModel):
            dream: str
            num_images: int
            image_size: int

        @app.post("/api/predict/")
        async def predict_api(data: Data):
            """Dream a dream. Defines the REST API which takes the text prompt, number of images and image size in the
            request body.

            This API returns an image generated by the model in base64 format.
            """
            try:
                result = pool.submit(self._scaled_model.predict, data.dream, data.num_images, data.image_size).result(
                    timeout=REQUEST_TIMEOUT
                )
                return result
            except TimeoutError:
                raise HTTPException(status_code=500, detail="Request timed out.")

        uvicorn.run(app, host=self.host, port=self.port)
